[tool.poetry]
name = "llm-serving-stack"
version = "0.1.0"
description = "Pluggable LLM serving stack with MLServer, UC hot-reload, streaming gateway, Prometheus & OpenTelemetry."
authors = ["Your Team <you@example.com>"]
license = "Apache-2.0"
readme = "README.md"
packages = [
  { include = "server" },
  { include = "gateway" }
]

[tool.poetry.dependencies]
python = ">=3.10,<3.13"

# Core serving
mlserver = "^1.4.0"
mlserver-mlflow = "^1.4.0"
mlflow = "^2.14.0"

# Observability
prometheus-client = "^0.20.0"
opentelemetry-sdk = "^1.27.0"
opentelemetry-exporter-otlp = "^1.27.0"

# Config / typing
pydantic = "^1.10.15"  # (runtime code uses Pydantic v1 APIs)
typing-extensions = "^4.12.2"

# Gateway
# Pin FastAPI to a Pydantic v1-compatible range (our code uses v1 BaseModel)
fastapi = ">=0.95,<0.100"
uvicorn = "^0.30.0"

# gRPC bridge
grpcio = "^1.63.0"
mlserver-grpc-protos = "^1.4.0"

# LLM adapters (optional but commonly used)
llama-index = "^0.11.0"
langchain = "^0.2.11"
langchain-core = "^0.2.27"
langchain-openai = "^0.1.17"

[tool.poetry.group.dev.dependencies]
pytest = "^8.2.0"
pytest-asyncio = "^0.23.8"
pytest-cov = "^5.0.0"
httpx = "^0.27.0"

[build-system]
requires = ["poetry-core>=1.8.0"]
build-backend = "poetry.core.masonry.api"
