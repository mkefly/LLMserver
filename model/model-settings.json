{
  "name": "llm-unified",
  "implementation": "server.runtime.LLMUnifiedRuntime",
  "parameters": {
    "adapter": "llama_index",
    "engine_type": "chat",
    "resolver": "uc_stage",
    "model_ref": "models:/main.default.research_index/Production",
    "top_k": 4,
    "memory_backend": "inproc",
    "timeout_s": 120,
    "stream_format": "text",
    "hot_reload": true,
    "hot_reload_interval_s": 30,
    "max_concurrent_streams": 64,
    "drain_seconds": 10
  }
}