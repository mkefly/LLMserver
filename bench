import asyncio
import select
import signal
import inspect
import os
import time
from asyncio import Task, CancelledError
from multiprocessing import Process, Queue
from concurrent.futures import ThreadPoolExecutor
from contextlib import nullcontext
from typing import Optional

from mlserver.registry import MultiModelRegistry
from mlserver.utils import install_uvloop_event_loop, schedule_with_callback
from mlserver.logging import configure_logger
from mlserver.settings import Settings
from mlserver.metrics import configure_metrics
from mlserver.context import model_context
from mlserver.env import Environment

from .messages import (
    ModelRequestMessage,
    ModelUpdateType,
    ModelUpdateMessage,
    ModelResponseMessage,
    # streaming message types
    ModelStreamChunkMessage,
    ModelStreamEndMessage,
)
from .utils import terminate_queue, END_OF_QUEUE
from .logging import logger
from .errors import WorkerError

IGNORED_SIGNALS = [signal.SIGINT, signal.SIGTERM, signal.SIGQUIT]


def _noop():
    pass


class Worker(Process):
    def __init__(
        self, settings: Settings, responses: Queue, env: Optional[Environment] = None
    ):
        super().__init__()
        self._settings = settings
        self._responses = responses
        self._requests: Queue[ModelRequestMessage] = Queue()
        self._model_updates: Queue[ModelUpdateMessage] = Queue()
        self._env = env

        self.__executor = None

        # ── NEW: activity counters & heartbeat
        self._req_total = 0
        self._req_stream_total = 0
        self._chunks_total = 0
        self._hb_task: Optional[asyncio.Task] = None
        self._active = False  # will flip true in __inner_init__

    # ---------- helpers -------------------------------------------------------

    def _logp(self) -> str:
        """Consistent prefix for worker logs."""
        return f"[worker name={self.name} pid={os.getpid()}]"

    def _safe_qsize(self, q: Queue) -> Optional[int]:
        try:
            return q.qsize()
        except (NotImplementedError, OSError):
            return None

    @property
    def _executor(self):
        # lazy threadpool (cannot be pickled pre-fork)
        if self.__executor is None:
            self.__executor = ThreadPoolExecutor()
        return self.__executor

    # ---------- process lifecycle --------------------------------------------

    def run(self):
        ctx = self._env or nullcontext()
        with ctx:
            install_uvloop_event_loop()
            configure_logger(self._settings)
            configure_metrics(self._settings)
            self._ignore_signals()
            pref = self._logp()
            try:
                logger.info(
                    "%s starting; parallel_workers=%s debug=%s",
                    pref,
                    getattr(self._settings, "parallel_workers", None),
                    getattr(self._settings, "debug", None),
                )
                asyncio.run(self.coro_run())
            finally:
                logger.info("%s exiting run()", pref)

    def _ignore_signals(self):
        loop = asyncio.get_event_loop()
        for sign in IGNORED_SIGNALS:
            loop.add_signal_handler(sign, _noop)

    def __inner_init__(self):
        self._model_registry = MultiModelRegistry()
        self._active = True

    async def coro_run(self):
        self.__inner_init__()
        loop = asyncio.get_event_loop()

        # ── NEW: start heartbeat
        self._hb_task = asyncio.create_task(self._heartbeat(), name="worker-heartbeat")

        pref = self._logp()
        logger.debug(
            "%s event loop started; requests_qsize=%s updates_qsize=%s",
            pref,
            self._safe_qsize(self._requests),
            self._safe_qsize(self._model_updates),
        )

        while self._active:
            readable = await loop.run_in_executor(self._executor, self._select)
            for r in readable:
                if r is self._requests._reader:
                    request = self._requests.get()
                    logger.debug(
                        "%s got request: id=%s model=%s v=%s method=%s",
                        pref,
                        getattr(request, "id", None),
                        getattr(request, "model_name", None),
                        getattr(request, "model_version", None),
                        getattr(request, "method_name", None),
                    )
                    schedule_with_callback(
                        self._process_request(request), self._handle_response
                    )
                elif r is self._model_updates._reader:
                    model_update = self._model_updates.get()
                    if model_update is END_OF_QUEUE:
                        logger.info("%s model_updates: end-of-queue received", pref)
                        self._active = False
                        break

                    logger.info(
                        "%s model update: type=%s name=%s version=%s",
                        pref,
                        getattr(model_update, "update_type", None),
                        getattr(getattr(model_update, "model_settings", None), "name", None),
                        getattr(getattr(model_update, "model_settings", None), "version", None),
                    )
                    schedule_with_callback(
                        self._process_model_update(model_update), self._handle_response
                    )

        # graceful shutdown
        if self._hb_task:
            self._hb_task.cancel()
            with contextlib.suppress(Exception):
                await self._hb_task

        logger.info("%s loop terminated cleanly", pref)

    def _select(self):
        readable, _, _ = select.select(
            [self._requests._reader, self._model_updates._reader], [], []
        )
        return readable

    # ---------- request & update handling -------------------------------------

    async def _process_request(self, request) -> ModelResponseMessage:
        pref = self._logp()
        t0 = time.perf_counter()
        try:
            model = await self._model_registry.get_model(
                request.model_name, request.model_version
            )

            method = getattr(model, request.method_name)
            with model_context(model.settings):
                result = method(*request.method_args, **request.method_kwargs)

                # Await plain awaitables (but not async generators)
                if inspect.isawaitable(result) and not inspect.isasyncgen(result):
                    result = await result

                # STREAMING PATH
                if inspect.isasyncgen(result) or hasattr(result, "__aiter__"):
                    self._req_total += 1
                    self._req_stream_total += 1
                    chunks = 0
                    logger.debug(
                        "%s streaming start id=%s model=%s method=%s",
                        pref,
                        request.id,
                        request.model_name,
                        request.method_name,
                    )
                    try:
                        async for chunk in result:
                            chunks += 1
                            self._chunks_total += 1
                            # emit chunk
                            self._responses.put(
                                ModelStreamChunkMessage(id=request.id, chunk=chunk)
                            )
                            if chunks <= 3 or (chunks % 50 == 0):
                                # first few chunks at DEBUG; then every 50th to avoid noise
                                logger.debug(
                                    "%s streaming chunk id=%s count=%d", pref, request.id, chunks
                                )
                        # normal end
                        self._responses.put(ModelStreamEndMessage(id=request.id))
                        dt = (time.perf_counter() - t0) * 1000
                        logger.info(
                            "%s streaming end id=%s chunks=%d dur_ms=%.1f",
                            pref,
                            request.id,
                            chunks,
                            dt,
                        )
                    except (Exception, CancelledError) as e:
                        logger.exception(
                            "%s streaming error id=%s model=%s method=%s",
                            pref,
                            request.id,
                            request.model_name,
                            request.method_name,
                        )
                        self._responses.put(
                            ModelStreamEndMessage(id=request.id, exception=WorkerError(e))
                        )
                    # Return a minimal response to complete the scheduled task.
                    return ModelResponseMessage(id=request.id, return_value=None)

                # UNARY PATH
                self._req_total += 1
                dt = (time.perf_counter() - t0) * 1000
                logger.debug(
                    "%s unary done id=%s model=%s method=%s dur_ms=%.1f",
                    pref,
                    request.id,
                    request.model_name,
                    request.method_name,
                    dt,
                )
                return ModelResponseMessage(id=request.id, return_value=result)

        except (Exception, CancelledError) as e:
            logger.exception(
                "%s error calling method='%s' model='%s' id=%s",
                pref,
                getattr(request, "method_name", None),
                getattr(request, "model_name", None),
                getattr(request, "id", None),
            )
            worker_error = WorkerError(e)
            return ModelResponseMessage(id=request.id, exception=worker_error)

    def _handle_response(self, process_task: Task):
        # Called after scheduled coroutine resolves (unary or streaming sentinel)
        try:
            response_message = process_task.result()
        except Exception:
            logger.exception("%s _handle_response: task raised", self._logp())
            return
        self._responses.put(response_message)

    async def _process_model_update(
        self, update: ModelUpdateMessage
    ) -> ModelResponseMessage:
        pref = self._logp()
        try:
            model_settings = update.model_settings
            if update.update_type == ModelUpdateType.Load:
                logger.info(
                    "%s loading model name=%s version=%s",
                    pref,
                    model_settings.name,
                    model_settings.version,
                )
                await self._model_registry.load(model_settings)
            elif update.update_type == ModelUpdateType.Unload:
                logger.info(
                    "%s unloading model name=%s version=%s",
                    pref,
                    model_settings.name,
                    model_settings.version,
                )
                await self._model_registry.unload_version(
                    model_settings.name, model_settings.version
                )
            else:
                logger.warning(
                    "%s unknown model update type=%s", pref, update.update_type
                )
            return ModelResponseMessage(id=update.id)
        except (Exception, CancelledError) as e:
            logger.exception(
                "%s error processing model update type=%s name=%s version=%s",
                pref,
                getattr(update, "update_type", None),
                getattr(getattr(update, "model_settings", None), "name", None),
                getattr(getattr(update, "model_settings", None), "version", None),
            )
            worker_error = WorkerError(e)
            return ModelResponseMessage(id=update.id, exception=worker_error)

    def send_request(self, request_message: ModelRequestMessage):
        # Note: both multiprocess- and thread-safe
        self._requests.put(request_message)

    def send_update(self, model_update: ModelUpdateMessage):
        self._model_updates.put(model_update)

    async def stop(self):
        # graceful shutdown
        self._active = False
        await terminate_queue(self._model_updates)
        self._model_updates.close()
        self._requests.close()
        self._executor.shutdown()

    # ---------- NEW: heartbeat -------------------------------------------------

    async def _heartbeat(self):
        """
        Periodically log worker activity & queue sizes.
        Interval defaults to 10s; override with env MLSERVER_WORKER_HEARTBEAT_SEC.
        """
        pref = self._logp()
        interval = float(os.getenv("MLSERVER_WORKER_HEARTBEAT_SEC", "10"))
        while True:
            try:
                await asyncio.sleep(interval)
                logger.info(
                    "%s heartbeat active=%s req_total=%d stream_reqs=%d chunks_total=%d "
                    "qsize{req=%s,updates=%s}",
                    pref,
                    self._active,
                    self._req_total,
                    self._req_stream_total,
                    self._chunks_total,
                    self._safe_qsize(self._requests),
                    self._safe_qsize(self._model_updates),
                )
            except asyncio.CancelledError:
                logger.debug("%s heartbeat cancelled", pref)
                break
            except Exception:
                logger.exception("%s heartbeat error", pref)
