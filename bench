gotcha — here’s a **drop-in replacement** that makes the streaming benchmark run **many cases**, not just one. It matches your repo’s structure (helpers + RestClient) and stays single-server.

---

# benchmarking/common/rest.js

(add `inferStream`; keep the rest as-is)

```js
// benchmarking/common/rest.js
import { check } from "k6";
import http from "k6/http";

function checkResponse(res) {
  check(res, {
    "is status 200": (r) => r.status === 200,
  });
}

export class RestClient {
  constructor() {
    this.restHost = `http://${__ENV.MLSERVER_HOST}:${__ENV.MLSERVER_HTTP_PORT}`;
    this.modelVersion = __ENV.MODEL_VERSION || "";
  }

  #versionsPrefix(name) {
    return this.modelVersion
      ? `/v2/models/${name}/versions/${encodeURIComponent(this.modelVersion)}`
      : `/v2/models/${name}`;
  }

  loadModel(name) {
    const res = http.post(`${this.restHost}/v2/repository/models/${name}/load`);
    checkResponse(res);
    return res;
  }

  unloadModel(name) {
    const res = http.post(`${this.restHost}/v2/repository/models/${name}/unload`);
    checkResponse(res);
    return res;
  }

  infer(name, payload) {
    const headers = { "Content-Type": "application/json" };
    const res = http.post(
      `${this.restHost}/v2/models/${name}/infer`,
      JSON.stringify(payload),
      { headers }
    );
    checkResponse(res);
    return res;
  }

  // NEW: stream endpoint
  inferStream(name, payload) {
    const headers = {
      "Content-Type": "application/json",
      Accept: "application/x-ndjson, application/json, */*",
    };
    const url = `${this.restHost}${this.#versionsPrefix(name)}/infer_stream`;
    const res = http.post(url, JSON.stringify(payload), { headers });
    checkResponse(res);
    return res;
  }
}
```

---

# benchmarking/scenarios/inference-rest-stream.js

(rewritten to **generate a bunch of cases** + also **use your data file**; it will iterate across **all** cases, not just one)

```js
// benchmarking/scenarios/inference-rest-stream.js
import { group } from "k6";
import { SharedArray } from "k6/data";
import { readTestData } from "../common/helpers.js";
import { RestClient } from "../common/rest.js";

/**
 * One server at a time.
 * Builds a LARGE case list from:
 *   1) ../data/<TEST_DATASET>/rest-requests.json  (single or array)
 *   2) OPTIONAL: generated cases (dtype/shape/mode grid)
 *
 * Env knobs:
 *   MLSERVER_HOST, MLSERVER_HTTP_PORT
 *   MODEL_NAME                (default: TEST_DATASET or "sum-model")
 *   TEST_DATASET              (folder under ../data/, default: "sum-model")
 *   PARALLEL_WORKERS          (just a tag; set to match your server config)
 *
 *   -- Generation controls (set GENERATE=1 to enable, default 1):
 *   GENERATE=1|0
 *   DATATYPES='["FP32","INT32","BYTES"]'
 *   SHAPES='[[5],[16],[2,8]]'
 *   MODES='["sequence","random","const"]'
 *   CASES_PER_COMBO=4
 *   CONST_VALUE=1
 *   BYTES_ENCODING=utf8|base64
 *   BYTES_WORD=token
 *   SEED=1337
 *
 *   Execution:
 *   VUS=8
 *   LOOPS=3                   (#times to traverse the whole case list)
 *   ITERATIONS                (override total iters; else LOOPS*cases)
 */

function jsonEnv(name, fallback) {
  const raw = __ENV[name];
  if (!raw) return fallback;
  try { return JSON.parse(raw); } catch (e) { throw new Error(`Invalid JSON in ${name}: ${e.message}`); }
}
function numEnv(name, def) {
  const raw = __ENV[name];
  if (raw == null || raw === "") return def;
  const n = Number(raw);
  if (!Number.isFinite(n)) throw new Error(`Invalid number in ${name}: ${raw}`);
  return n;
}
function strEnv(name, def) {
  const raw = __ENV[name];
  return (raw == null || raw === "") ? def : raw;
}

const TEST_DATASET = strEnv("TEST_DATASET", "sum-model");
const MODEL_NAME = strEnv("MODEL_NAME", TEST_DATASET);

// ── generation config
const GENERATE = strEnv("GENERATE", "1") === "1";
const DATATYPES = jsonEnv("DATATYPES", ["FP32", "INT32", "BYTES"]).map((s) => s.toUpperCase());
const SHAPES = jsonEnv("SHAPES", [[5], [16], [2, 8]]);
const MODES = jsonEnv("MODES", ["sequence", "random", "const"]).map((s) => s.toLowerCase());
const CASES_PER_COMBO = numEnv("CASES_PER_COMBO", 4);
const CONST_VALUE = (__ENV.CONST_VALUE != null) ? __ENV.CONST_VALUE : 1;
const BYTES_ENCODING = strEnv("BYTES_ENCODING", "utf8").toLowerCase(); // utf8|base64
const BYTES_WORD = strEnv("BYTES_WORD", "token");
const SEED = numEnv("SEED", 1337);

// ── deterministic RNG
function lcg(seed) {
  let s = (seed >>> 0) || 1;
  return function rand() {
    s = (Math.imul(1664525, s) + 1013904223) >>> 0;
    return s / 0x100000000;
  };
}
function sizeFromShape(shape) { return (shape || []).reduce((a, b) => a * b, 1) || 0; }
function genNumeric(n, mode, dtype, rand) {
  const out = new Array(n);
  if (mode === "const") {
    const v = Number(CONST_VALUE);
    for (let i = 0; i < n; i++) out[i] = v;
  } else if (mode === "random") {
    for (let i = 0; i < n; i++) out[i] = dtype.startsWith("FP") ? rand() : Math.floor(rand() * 1000) - 500;
  } else {
    for (let i = 0; i < n; i++) out[i] = i + 1;
  }
  if (dtype === "INT64" || dtype === "INT32") return out.map((x) => Math.trunc(x));
  return out.map(Number);
}
function asciiToBase64(s) {
  const alphabet = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/";
  const bytes = Array.from(s, (c) => c.charCodeAt(0) & 0xff);
  let out = "";
  for (let i = 0; i < bytes.length; i += 3) {
    const b1 = bytes[i], b2 = bytes[i + 1], b3 = bytes[i + 2];
    const x1 = b1 >> 2;
    const x2 = ((b1 & 3) << 4) | (b2 != null ? (b2 >> 4) : 0);
    const x3 = b2 != null ? ((b2 & 15) << 2) | (b3 != null ? (b3 >> 6) : 0) : 64;
    const x4 = b3 != null ? (b3 & 63) : 64;
    out += alphabet[x1] + alphabet[x2] + (x3 === 64 ? "=" : alphabet[x3]) + (x4 === 64 ? "=" : alphabet[x4]);
  }
  return out;
}
function genBytes(n, mode, rand) {
  const arr = new Array(n);
  if (mode === "const") {
    const s = String(CONST_VALUE);
    for (let i = 0; i < n; i++) arr[i] = s;
  } else if (mode === "random") {
    for (let i = 0; i < n; i++) {
      const suffix = Math.floor(rand() * 1e9).toString(36);
      arr[i] = `${BYTES_WORD}_${i}_${suffix}`;
    }
  } else {
    for (let i = 0; i < n; i++) arr[i] = `${BYTES_WORD}_${i + 1}`;
  }
  if (BYTES_ENCODING === "base64") return arr.map(asciiToBase64);
  return arr;
}
function buildRequest(dtype, shape, mode, rand) {
  const n = sizeFromShape(shape);
  const data = (dtype === "BYTES") ? genBytes(n, mode, rand) : genNumeric(n, mode, dtype, rand);
  return {
    id: "k6-stream-batch",
    inputs: [{ name: "a", shape, datatype: dtype, data }],
    parameters: { content_type: "application/json" },
  };
}

// ── Build all CASES up front (SharedArray) — contains BOTH file-based & generated
const CASES = new SharedArray("stream_cases", () => {
  const items = [];

  // 1) from data/<TEST_DATASET>/rest-requests.json
  try {
    const td = readTestData(TEST_DATASET);
    const fileCases = Array.isArray(td.rest) ? td.rest : [td.rest];
    for (let i = 0; i < fileCases.length; i++) {
      items.push({ payload: fileCases[i], tag: { source: "data", idx: String(i) } });
    }
  } catch (e) {
    // no data folder or invalid JSON — silently skip
  }

  // 2) generated cases (optional)
  if (GENERATE) {
    for (const dtype of DATATYPES) {
      for (const shape of SHAPES) {
        for (const mode of MODES) {
          for (let i = 0; i < CASES_PER_COMBO; i++) {
            const seed = (SEED * 131 + i * 977 + JSON.stringify(shape).length + dtype.length + mode.length) >>> 0;
            const rand = lcg(seed);
            const req = buildRequest(dtype, shape, mode, rand);
            items.push({
              payload: req,
              tag: {
                source: "gen",
                dtype,
                shape: JSON.stringify(shape),
                mode,
                case_id: `${dtype}-${JSON.stringify(shape)}-${mode}-${i}`,
              },
            });
          }
        }
      }
    }
  }

  return items;
});

// ── Execution plan: iterate over ALL cases (possibly multiple LOOPS)
const VUS = numEnv("VUS", 8);
const LOOPS = numEnv("LOOPS", 1);
const TOTAL_CASES = CASES.length * LOOPS;
const USER_ITER = __ENV.ITERATIONS ? numEnv("ITERATIONS", 0) : null;

export const options = {
  vus: VUS,
  iterations: USER_ITER != null ? USER_ITER : Math.max(1, TOTAL_CASES),
  tags: {
    scenario: "inference-rest-stream",
    model_name: MODEL_NAME,
    parallel_workers: __ENV.PARALLEL_WORKERS || "",
  },
};

const rest = new RestClient();

export function setup() {
  // If you want to ensure the model is resident before the test loop starts:
  rest.loadModel(MODEL_NAME);
  return { total: CASES.length };
}

export default function (data) {
  // Round-robin through the whole case list across LOOPS
  const idx = __ITER % CASES.length;
  const { payload, tag } = CASES[idx];

  group(`stream:${MODEL_NAME}`, () => {
    // Stream call — status checked in RestClient
    // (Optional) pass tags down to built-in http metrics via request-level tags:
    rest.inferStream(MODEL_NAME, payload);

    // If you want NDJSON chunk counting, you can re-issue with http.post here
    // and parse res.body; keeping it minimal & repo-style for now.
  });
}

export function teardown() {}
```

---

## how to run (examples)

**Use dataset + generated grid; iterate across ALL cases (default LOOPS=1):**

```bash
k6 run \
  -e MLSERVER_HOST=0.0.0.0 \
  -e MLSERVER_HTTP_PORT=8080 \
  -e MODEL_NAME=sum-model \
  -e TEST_DATASET=sum-model \
  -e PARALLEL_WORKERS=2 \
  benchmarking/scenarios/inference-rest-stream.js
```

**Bigger generated matrix (in addition to your data file), loop over it 3 times:**

```bash
DATATYPES='["FP32","INT32","BYTES"]' \
SHAPES='[[5],[16],[2,8]]' \
MODES='["sequence","random","const"]' \
CASES_PER_COMBO=5 \
LOOPS=3 \
VUS=16 \
k6 run \
  -e MLSERVER_HOST=0.0.0.0 \
  -e MLSERVER_HTTP_PORT=8080 \
  -e MODEL_NAME=sum-model \
  -e TEST_DATASET=sum-model \
  -e PARALLEL_WORKERS=4 \
  benchmarking/scenarios/inference-rest-stream.js
```

**Only use your data file (disable generator):**

```bash
GENERATE=0 \
k6 run \
  -e MLSERVER_HOST=0.0.0.0 \
  -e MLSERVER_HTTP_PORT=8080 \
  -e MODEL_NAME=sum-model \
  -e TEST_DATASET=sum-model \
  benchmarking/scenarios/inference-rest-stream.js
```

This recreates the benchmark so it **runs every case** (from your dataset and/or a generated grid) rather than just one.
